# -*- coding: utf-8 -*-
"""FCV Project Final Version.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Y4JDlbHMnsNkI6hEPgLtsq45Ny6tVH-

# Import and download data
"""

import numpy as np
import matplotlib.pyplot as plt
import os
import matplotlib.image as mpimg 
import math

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential,Model
from keras.layers import Dense, Dropout, Flatten, Reshape, InputLayer
from keras.layers.normalization import BatchNormalization
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.layers import Dense, Activation, Flatten, Conv2D, Conv2DTranspose, Dropout, MaxPooling2D,GlobalAveragePooling2D,GlobalAveragePooling1D
from keras import applications
import tensorflow as tf

from tensorflow.keras.applications import ResNet50
from keras.preprocessing.image import  ImageDataGenerator 
from keras.applications.resnet50 import preprocess_input

from sklearn.metrics import classification_report
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix

!pip3 install scikit-plot

# !gdown --id 1rXl6cSoxUS1BtOi0tHreiLmVCw6BdWgb #dataset full
# !gdown --id 1tupq1BBr5YBdMKAOZa9o8nOcmDXOEXU6 #dataset.txt
# !gdown --id 1ODNbd8Nux85hZ4u7BzxwfuPU4aiJeuIx #dataset train & test
# !gdown --id 1Gc1DxZLiTUg5DClnwp78NIIOEVuTRtqo #dataset cropped & uncropped train & test
# !gdown --id 1uy4iB8L51gj3Ij1TgPTeEo_w41YZu_Hc #dataset just cropped train & test
# !gdown --id 1oEAIGg69RBWVYK4l4GUB7ah2tV2R4pqv #dataset just cropped labeled names train & test
# !gdown --id 1SVG-OfLVQdC1e7p0n5UwN_zZkjb6r5IW #dataset just cropped manually augmented train & test
!gdown --id 1-SfOiMVPogJfucwua60VMoqadCNONZ9s #dataset just cropped manually augmented train & test

# !unzip plate_dataset.zip
# !unzip plate_dataset_train_test.zip
# !unzip plate_dataset_cropped_uncropped_train_test.zip
# !unzip plate_dataset_cropped_train_test.zip
# !unzip plate_dataset_train_test_manualcropped.zip
!unzip plate_dataset_train_test_manual_cropped_merged.zip

"""## An example from dataset"""

# BASE_PATH = '/content/drive/MyDrive/stanford_car_dataset/'
BASE_PATH = 'new_added_dataset'

# path to the train folder
DATA_TRAIN_PATH = os.path.join(BASE_PATH, 'train')

# path to the test folder 
DATA_TEST_PATH = os.path.join(BASE_PATH, 'test')

image = mpimg.imread(os.path.join(DATA_TRAIN_PATH, '1/0001.jpg'))

print(image.shape)

plt.imshow(image)
plt.axis('off')
plt.show()

"""# Plotter"""

"""## Plot"""
def HistoryPlotter(history):
    # print(history.history.keys())
    plt.title('model accuracy')
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()

    plt.title('model loss')
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()


def CompareResult(x, y, y_predicted):
    plt.scatter(x, y, label='Actual')
    plt.scatter(x ,y_predicted, label='Predicted')
    plt.title('Train Data')
    plt.xlabel('Input Variable (x)')
    plt.ylabel('Output Variable (y)')
    plt.legend()
    plt.show()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline  
import scikitplot as skplt


def PlotConfusionMatrix(class_labels, true_classes):
    [print(k, ":", v) for k,v in enumerate(class_labels)]
    true_map_classes = [class_labels[x] for x in true_classes]
    predicted_map_classes = [class_labels[x] for x in predicted_classes]

    skplt.metrics.plot_confusion_matrix(
        true_map_classes, 
        predicted_map_classes,
        labels=class_labels,
        x_tick_rotation=90,
        figsize=(12,12))

from sklearn.metrics import classification_report


def PlotMany(predicted_classes, true_classes, predictions):
    report = classification_report(
        true_classes,
        predicted_classes,
        target_names=class_labels)
    print(report)

"""# Resnet

## set hyperparameters
"""

IMG_WIDTH, IMG_HEIGHT = 224, 224
EPOCHS = 5
BATCH_SIZE= 16
VERBOSITY = 2
n_classes = 2
n_channels = 3
VERBOSITY = 1

from keras.applications.vgg16 import preprocess_input
# train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)



train_datagen = ImageDataGenerator(rotation_range=20,brightness_range=[0.5,1.5])

#test_generator
test_datagen = ImageDataGenerator(
    rotation_range=20,brightness_range=[0.5,1.5])

train_generator = train_datagen.flow_from_directory(DATA_TRAIN_PATH,
                                                     target_size=(IMG_WIDTH, IMG_HEIGHT),
                                                     batch_size=BATCH_SIZE,
                                                     class_mode='binary')

test_generator = test_datagen.flow_from_directory(DATA_TEST_PATH,
                                                   target_size=(IMG_WIDTH, IMG_HEIGHT),
                                                   batch_size=BATCH_SIZE,
                                                   class_mode='binary')

"""## build and train model"""

def build_model_resnet50():
    model = Sequential()
    
    base_model = ResNet50(weights='imagenet', include_top=False, pooling ='avg')
    model.add(base_model)
    model.layers[0].trainable = False
    print("\nlayers***********",len(model.layers),"\n")
    # for layer in model.layers[:40]:
    #     layer.trainable = False
    
    model.add(Dense(1024, activation = 'relu')) 

    # x = base_model.output
    # x = GlobalAveragePooling2D()(x)
    # x = Dropout(0.7)(x)
    # predictions = Dense(num_classes, activation= 'softmax')(x)
    # model = Model(inputs = base_model.input, outputs = predictions)

    # model.add(GlobalAveragePooling1D())
    model.add(Dropout(0.7))
    model.add(Dense(1, activation = 'softmax')) 
    return model

def build_model_CNN():
    model = Sequential()

    model.add(InputLayer(input_shape=(IMG_WIDTH, IMG_HEIGHT, n_channels)))

    model.add(BatchNormalization())

    model.add(Conv2D(filters=64, kernel_size=(4, 4), padding='same', activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Dropout(0.2))

    model.add(Conv2D(filters=64, kernel_size=(4, 4), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Dropout(0.35))

    model.add(Flatten())

    model.add(Dense(256, activation='relu'))

    model.add(Dropout(0.5))

    model.add(Dense(64, activation='relu'))

    model.add(BatchNormalization())

    model.add(Dense(1, activation='softmax'))
    return model

# def build_model_c():

# model = build_model_with_resnet50()
from keras.optimizers import SGD,Adam

# model = build_model_CNN()
model = build_model_resnet50()

loss = 'binary_crossentropy' 
optimizer = Adam(lr=1e-4)
# optimizer = SGD(lr=0.000001)
# optimizer = 'adam'


# class_weight = {0 : 0.13, 1 : 0.87}
# model.compile(loss= loss, optimizer= optimizer, metrics=['accuracy'])
model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate
              loss=keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=[keras.metrics.BinaryAccuracy()])
history = model.fit(train_generator, epochs=EPOCHS, batch_size=BATCH_SIZE, 
                    # class_weight=class_weight, 
                    verbose=VERBOSITY,
                          validation_data=test_generator)

# model.fit(train_generator_aug, epochs=EPOCHS, batch_size=BATCH_SIZE, class_weight=class_weight)
HistoryPlotter(history)

"""#Mobile net"""

img_height,img_width = 224,224
num_classes = 1

base_model = tf.keras.applications.MobileNetV2(input_shape=(img_height,img_width,3),include_top=False,weights='imagenet')
base_model.trainable = True

inputs = keras.Input(shape=(img_height,img_width,3))
x = base_model(inputs, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
x = keras.layers.Dense(64,activation= 'relu',kernel_initializer='he_uniform')(x)
x = Dropout(0.7)(x)
x = keras.layers.Dense(16,activation= 'relu',kernel_initializer='he_uniform')(x)
outputs = keras.layers.Dense(1,activation= 'sigmoid')(x)
model_D = keras.Model(inputs, outputs)

model_D.summary()

from keras.optimizers import SGD, Adam
# sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)
adam = Adam(lr=0.0001)
# model_D.compile(optimizer= adam, loss='binary_crossentropy', metrics=['accuracy'])
base_learning_rate = 0.0001

model_D.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),

              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),

              metrics=['accuracy','MeanSquaredError'])

from keras.callbacks import EarlyStopping

es = EarlyStopping(monitor='val_loss', mode='min',verbose = 1
                   ,patience=5
                   )



history = model_D.fit(train_generator, epochs=20, batch_size=16, 
                    # class_weight=class_weight, 
                    verbose=VERBOSITY,
                          validation_data=test_generator)
                    #   ,callbacks=[es])

# model.fit(train_generator_aug, epochs=EPOCHS, batch_size=BATCH_SIZE, class_weight=class_weight)
HistoryPlotter(history)

model_D.evaluate(train_generator)

image = mpimg.imread(os.path.join(DATA_TRAIN_PATH, '1/0001.jpg'))
plt.imshow(image)
image = np.resize(image,(224,224,3))
image = np.expand_dims(image, axis=0)
print(np.shape(image))
print("pre",model_D.predict(image))
print(image.shape)
plt.axis('off')
plt.show()

predictions = model_D.predict_generator(
    train_generator,
    verbose=1,
    steps=math.ceil(train_generator.samples/train_generator.batch_size))
predicted_classes = np.argmax(predictions, axis=1) 

true_classes = train_generator.classes
class_labels = list(train_generator.class_indices.keys())

PlotMany(predicted_classes, true_classes, predictions)
PlotConfusionMatrix(class_labels, true_classes)

# model.evaluate(test_generator, batch_size=BATCH_SIZE)
model_pred = model.predict_generator(test_generator, steps=2)

# y_pred_bool = np.argmax(y_pred, axis=1)
f1 = f1_score(test_generator, model_pred, average='macro')
print(f1)

print(classification_report(y_test, y_pred_bool))

